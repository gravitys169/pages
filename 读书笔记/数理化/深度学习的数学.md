
## 导数

![](../attachments/Pasted%20image%2020240425100735.png)
  
常用导数公式

![](../attachments/Pasted%20image%2020240425100835.png)

偏导数

![](../attachments/Pasted%20image%2020240425101017.png)

#### 链式法则

![](../attachments/Pasted%20image%2020240425101130.png)

#### 多变量函数的近似公式

###### 单变量：从导数定义到近似公式
![](../attachments/Pasted%20image%2020240718155853.png)

![](../attachments/Pasted%20image%2020240718161901.png)

![](../attachments/Pasted%20image%2020240718161931.png)

###### 多变量
- ![](../attachments/Pasted%20image%2020240425101330.png)
- ![](../attachments/Pasted%20image%2020240718162022.png)

![](../attachments/20240718155731.jpg)

#### **梯度下降法**

由![](../attachments/Pasted%20image%2020240718162300.png)
可知，可表示为向量内积，使用向量的余弦距离公式（a*b=|a|b|cosx, x=180度时，内积最小）
- ![](../attachments/Pasted%20image%2020240718162514.png)
- ![](../attachments/Pasted%20image%2020240425101500.png)

两个向量方向相反时，梯度最大

![](../attachments/20240718162613.jpg)

![](../attachments/Pasted%20image%2020240425101520.png)

**通过梯度更新 delta x 和 delta y，然后得到新的 x 和 y，直到 x 和 y 不再变化（或经过一定迭代次数，此时梯度趋近于0），函数取得极值**

**梯度下降法的 excel 演示**

![](../attachments/Pasted%20image%2020240925115148.png)

**回归分析**

![](../attachments/Pasted%20image%2020240425102139.png)

![](../attachments/Pasted%20image%2020240425102127.png)

基于以下公式，联立方程求得对应参数p,q

![](../attachments/Pasted%20image%2020240425102212.png)

**求最小值的两种方案**
1. 根据输入数据，按照各变量偏导数为 0，联立各参数求解方程，得出参数，不过条件的数据规模要大于模型参数的个数，方程方可求解
2. 因联立求解方程困难，使用梯度下降法，逐步逼近最小值

神经网络

![](../attachments/Pasted%20image%2020240425103134.png)
![](../attachments/20240425104023.jpg)

矩阵表示：

![](../attachments/Pasted%20image%2020240506191116.png)

代价函数

![](../attachments/20240425103715.jpg)
- 方法 1：通过偏导数为0，求解参数方程（十分困难）

![](../attachments/Pasted%20image%2020240425103222.png)

- 方法 2：梯度下降法求解

![](../attachments/Pasted%20image%2020240425103409.png)

相对求解参数方程已相对容易许多，但依然需要求解代价函数相对参数的偏导，这里依然困难：

![](../attachments/Pasted%20image%2020240425103600.png)

![](../attachments/20240425104023.jpg)

通过求解器计算 loss

![](../attachments/Pasted%20image%2020240925115559.png)
#### 反向传播
神经单元误差delta

![](../attachments/Pasted%20image%2020240425104140.png)

![](../attachments/20240425104213.jpg)

delta与权重参数偏导的关系

![](../attachments/20240425104349.jpg)

![](../attachments/Pasted%20image%2020240425104722.png)

输出层的delta误差

![](../attachments/Pasted%20image%2020240425104839.png)

#### L与L+1层的递推公式：

![](../attachments/20240606172500.jpg)

由上图可知
![](../attachments/Pasted%20image%2020240606172518.png)

根据 ![](../attachments/Pasted%20image%2020240606172741.png)  ![](../attachments/Pasted%20image%2020240606172756.png)  ![](../attachments/Pasted%20image%2020240606172805.png)

可得： ![](../attachments/Pasted%20image%2020240606172640.png)

一般的：

![](../attachments/Pasted%20image%2020240425104946.png)

利用误差反向传播法确定神经网络的权重和偏置的算法：

![](../attachments/Pasted%20image%2020240925115800.png)

###### Excel 演示反向传播：

关键公式：

![](../attachments/Pasted%20image%2020240425104722.png)

![](../attachments/Pasted%20image%2020240425104946.png)

![](../attachments/Pasted%20image%2020240925120003.png)
## 卷积神经网络

![](../attachments/20240425105321.jpg)

卷积运算

![](../attachments/20240425105631.jpg)

一般的：

![](../attachments/Pasted%20image%2020240425110941.png)

池化层

![](../attachments/20240425110049.jpg)

![](../attachments/Pasted%20image%2020240425111007.png)

输出层

![](../attachments/20240425110128.jpg)

![](../attachments/Pasted%20image%2020240425111024.png)

代价函数

![](../attachments/Pasted%20image%2020240425110307.png)

梯度下降法

![](../attachments/Pasted%20image%2020240425111230.png)

delta与卷积层偏导数

![](../attachments/20240425111525.jpg)

输出层的梯度分量

![](../attachments/Pasted%20image%2020240425111715.png)

![](../attachments/Pasted%20image%2020240425112141.png)

卷积层梯度分量

![](../attachments/20240425112043.jpg)

![](../attachments/Pasted%20image%2020240425112222.png)

![](../attachments/Pasted%20image%2020240425112229.png)

#### 反向传播

输出层delta

![](../attachments/Pasted%20image%2020240425112501.png)

![](../attachments/Pasted%20image%2020240425112347.png)

反向递推

![](../attachments/20240425112553.jpg)

一般的：只需要求出输出层的神经单元误差delta，就可求出卷积层的误差delta

![](../attachments/Pasted%20image%2020240425112803.png)

例：

![](../attachments/Pasted%20image%2020240425113138.png)

![](../attachments/20240425113008.jpg)

### 附件

[图灵社区](http://www.ituring.com.cn/book/2593)