
## 神经网络复习
- 神经网络具有输入层、隐藏层和输出层
- 通过全连接层进行线性变换，通过激活函数进行非线性变换
- 全连接层和 mini-batch 处理都可以写成矩阵计算
- 使用误差反向传播法可以高效地求解神经网络的损失的梯度
- 使用计算图能够将神经网络中发生的处理可视化，这有助于理解正向传播和反向传播
- 在神经网络的实现中，通过将组件模块化为层，可以简化实现
- 数据的位精度和 GPU 并行计算对神经网络的高速化非常重要

#### 典型训练代码
```python
import sys
sys.path.append（'..'）
import numpy as np
from common.optimizer import SGD
from dataset import spiral
import matplotlib.pyplot as plt
from two layer net import TwoLayerNet

# ❶设定超参数
max_epoch = 300
batch_size = 30
hidden_size = 10
learning_rate = 1.0

# ❷读入数据，生成模型和优化器
x, t = spiral.load_data（）
model = TwoLayerNet（input_size=2, hidden_size=hidden_size, output_size=3）
optimizer = SGD（lr=learning_rate）

# 学习用的变量
data_size = len（x）
max_iters = data_size // batch_size
total_loss = 0
loss_count = 0
loss_list = []
for epoch in range（max_epoch）:
    # ❸打乱数据
    idx = np.random.permutation（data_size）
    x = x[idx]
    t = t[idx]

    for iters in range（max_iters）:
        batch_x = x[iters*batch_size:（iters+1）*batch_size]
        batch_t = t[iters*batch_size:（iters+1）*batch_size]

    # ❹计算梯度，更新参数
    loss = model.forward（batch_x, batch_t）
    model.backward（）
    optimizer.update（model.params, model.grads）
    total_loss += loss
    loss_count += 1

    # ❺定期输出学习过程
    if （iters+1） % 10 == 0:
        avg_loss = total_loss / loss_count
        print （'| epoch %d |  iter %d / %d | loss %.2f'
              % （epoch + 1, iters + 1, max_iters, avg_loss））
        loss_list.append（avg_loss）
        total_loss, loss_count = 0, 0
```

## 分词表示
不仅限于自然语言处理，在图像识别领域，多年来也一直是人工设计特征量。但是，随着深度学习的出现，现在从原始图像直接获得最终结果已成为可能，人为介入的必要性大幅降低。在自然语言处理领域也有类似现象。也就是说，我们正在从人工制作词典或设计特征量的旧范式，向尽量减少人为干预的、仅从文本数据中获取最终结果的新范式转移。

#### 基于统计的分词表示

共现矩阵

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202406240946768.png?token=ACGIA7UFULLIR2HNZEMI6Q3GPDIE2)

向量相似度度量

余弦相似度

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202406240947602.png?token=ACGIA7ST4QFJKLTWDVG3YH3GPDII2)

点互信息（Pointwise Mutual Information, PMI）

考虑单个词出现的概率，以避免常用词的干扰

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202406240948134.png?token=ACGIA7QTGOGWIR33WQJHIWTGPDILY)

考虑到两个单词的共现次数为 0 时，log<sub>2</sub> <sup>0</sup>=-∞。为了解决这个问题，实践上我们会使用下述正的点互信息（Positive PMI, PPMI）。

**降维**

就是减少向量维度

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202406240950305.png?token=ACGIA7QHVCPIA6UCPPDN6CDGPDIRQ)

奇异值分解

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202406240950851.png?token=ACGIA7QNMQZA62U6POXDG33GPDITW)

#### 小结

- 使用 WordNet 等同义词词典，可以获取近义词或测量单词间的相似度等
- 使用同义词词典的方法存在创建词库需要大量人力、新词难更新等问题
- 目前，使用语料库对单词进行向量化是主流方法
- 近年来的单词向量化方法大多基于“单词含义由其周围的单词构成”这一分布式假设
- 在基于计数的方法中，对语料库中的每个单词周围的单词的出现频数进行计数并汇总（=共现矩阵）
- 通过将共现矩阵转化为 PPMI 矩阵并降维，可以将大的稀疏向量转变为小的密集向量
- 在单词的向量空间中，含义上接近的单词距离上理应也更近