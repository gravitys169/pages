求最小值的两种方案：
1. 根据输入数据，按照各变量偏导数为0，联立各参数求解方程，得出参数
2. 因联立求解方程困难，使用梯度下降法，逐步逼近最小值

导数：
![](attachments/Pasted%20image%2020240425100735.png)
常用导数公式
![](attachments/Pasted%20image%2020240425100835.png)
偏导数
![](attachments/Pasted%20image%2020240425101017.png)
链式法则
![](attachments/Pasted%20image%2020240425101130.png)
多变量函数的近似公式
![](attachments/Pasted%20image%2020240425101330.png)
![](attachments/Pasted%20image%2020240425101409.png)
梯度下降法：
这里使用了向量的余弦距离公式
![](attachments/Pasted%20image%2020240425101500.png)
![](attachments/Pasted%20image%2020240425101520.png)
回归分析
![](attachments/Pasted%20image%2020240425102139.png)
![](attachments/Pasted%20image%2020240425102127.png)
基于以下公式，联立方程求得对应参数p,q
![](attachments/Pasted%20image%2020240425102212.png)
条件的数据规模要大于模型参数的个数，方程方可求解

神经网络
![](attachments/Pasted%20image%2020240425103134.png)
代价函数
![](attachments/20240425103715.jpg)
- 通过偏导数为0，求解参数方程十分困难
![](attachments/Pasted%20image%2020240425103222.png)
- 梯度下降法求解
![](attachments/Pasted%20image%2020240425103409.png)
相对求解参数方程已相对容易很大，但依然需要求解代价函数相对参数的偏导，这里依然困难：
![](attachments/Pasted%20image%2020240425103600.png)
![](attachments/20240425104023.jpg)

神经单元误差
![](attachments/Pasted%20image%2020240425104140.png)
![](attachments/20240425104213.jpg)
delta与权重参数偏导的关系
![](attachments/20240425104349.jpg)
![](attachments/Pasted%20image%2020240425104722.png)

输出层的delta误差
![](attachments/Pasted%20image%2020240425104839.png)
L与L+1层的递推公式：
![](attachments/Pasted%20image%2020240425104946.png)

## 卷积神经网络
![](attachments/20240425105321.jpg)
卷积运算
![](attachments/20240425105631.jpg)
一般的：![](attachments/Pasted%20image%2020240425110941.png)
池化层
![](attachments/20240425110049.jpg)
![](attachments/Pasted%20image%2020240425111007.png)
输出层
![](attachments/20240425110128.jpg)
![](attachments/Pasted%20image%2020240425111024.png)

代价函数
![](attachments/Pasted%20image%2020240425110307.png)



### 附件
[图灵社区](http://www.ituring.com.cn/book/2593)