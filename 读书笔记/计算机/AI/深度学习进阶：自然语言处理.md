
## 神经网络复习
- 神经网络具有输入层、隐藏层和输出层
- 通过全连接层进行线性变换，通过激活函数进行非线性变换
- 全连接层和 mini-batch 处理都可以写成矩阵计算
- 使用误差反向传播法可以高效地求解神经网络的损失的梯度
- 使用计算图能够将神经网络中发生的处理可视化，这有助于理解正向传播和反向传播
- 在神经网络的实现中，通过将组件模块化为层，可以简化实现
- 数据的位精度和 GPU 并行计算对神经网络的高速化非常重要

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202407151404317.png)

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202407151405867.png)

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202407151407492.png)

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202407151407164.png)

仿射变换是一种可以由矩阵乘法和向量加法表示的变换，它包括平移、缩放、旋转、翻转和剪切等基本变换的组合。
仿射变换可以表示为 𝑇(𝑥)=𝐴𝑥+𝑏T(x)=Ax+b，其中 𝐴A 是一个非奇异的线性变换矩阵，𝑥x 是原始向量，𝑏b 是一个平移向量。
#### 典型训练代码
```python
import sys
sys.path.append（'..'）
import numpy as np
from common.optimizer import SGD
from dataset import spiral
import matplotlib.pyplot as plt
from two layer net import TwoLayerNet

# ❶设定超参数
max_epoch = 300
batch_size = 30
hidden_size = 10
learning_rate = 1.0

# ❷读入数据，生成模型和优化器
x, t = spiral.load_data（）
model = TwoLayerNet（input_size=2, hidden_size=hidden_size, output_size=3）
optimizer = SGD（lr=learning_rate）

# 学习用的变量
data_size = len（x）
max_iters = data_size // batch_size
total_loss = 0
loss_count = 0
loss_list = []
for epoch in range（max_epoch）:
    # ❸打乱数据
    idx = np.random.permutation（data_size）
    x = x[idx]
    t = t[idx]

    for iters in range（max_iters）:
        batch_x = x[iters*batch_size:（iters+1）*batch_size]
        batch_t = t[iters*batch_size:（iters+1）*batch_size]

    # ❹计算梯度，更新参数
    loss = model.forward（batch_x, batch_t）
    model.backward（）
    optimizer.update（model.params, model.grads）
    total_loss += loss
    loss_count += 1

    # ❺定期输出学习过程
    if （iters+1） % 10 == 0:
        avg_loss = total_loss / loss_count
        print （'| epoch %d |  iter %d / %d | loss %.2f'
              % （epoch + 1, iters + 1, max_iters, avg_loss））
        loss_list.append（avg_loss）
        total_loss, loss_count = 0, 0
```

## 自然语言的分词表示
不仅限于自然语言处理，在图像识别领域，多年来也一直是人工设计特征量。但是，随着深度学习的出现，现在从原始图像直接获得最终结果已成为可能，人为介入的必要性大幅降低。在自然语言处理领域也有类似现象。也就是说，我们正在从人工制作词典或设计特征量的旧范式，向尽量减少人为干预的、仅从文本数据中获取最终结果的新范式转移。

#### 基于统计的分词表示

单词的分布式假设：单词本身没有含义，单词含义由它所在的上下文（语境）形成---这是不是可以作为看文章不必关注某个生词的理由？可以，但是得提供可以类比的上下文，方可推导出来。

共现矩阵

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202406240946768.png?token=ACGIA7UFULLIR2HNZEMI6Q3GPDIE2)

#### 向量相似度度量

余弦相似度

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202406240947602.png?token=ACGIA7ST4QFJKLTWDVG3YH3GPDII2)

#### 点互信息（Pointwise Mutual Information, PMI）

why PMI：考虑单个词出现的概率，以避免常用词的干扰

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202406240948134.png?token=ACGIA7QTGOGWIR33WQJHIWTGPDILY)

![](../../attachments/Pasted%20image%2020240925165644.png)

其中，将共现矩阵表示为 C，将单词 x 和 y 的共现次数表示为 C（x, y）​，将单词 x 和 y 的出现次数分别表示为 C（x）​、C（y）​，将语料库的单词数量记为 N

考虑到两个单词的共现次数为 0 时，log<sub>2</sub> <sup>0</sup>=-∞。为了解决这个问题，实践上我们会使用下述正的点互信息（Positive PMI, PPMI）。

![](../../attachments/Pasted%20image%2020240925162331.png)

PPMI 矩阵存在一个很大的问题，那就是随着语料库的词汇量增加，各个单词向量的维数也会增加。如果语料库的词汇量达到 10 万，则单词向量的维数也同样会达到 10 万（其中很多维为 0）。实际上，处理 10 万维向量是不现实的。

#### 降维

就是减少向量维度

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202406240950305.png?token=ACGIA7QHVCPIA6UCPPDN6CDGPDIRQ)

向量中的大多数元素为 0 的矩阵（或向量）称为稀疏矩阵（或稀疏向量）​。这里的重点是，从稀疏向量中找出重要的轴，用更少的维度对其进行重新表示。结果，稀疏矩阵就会被转化为大多数元素均不为 0 的密集矩阵。这个密集矩阵就是我们想要的单词的分布式表示。

奇异值分解（Singular Value Decomposition）

![](https://raw.githubusercontent.com/gravitys169/images_upload/master/202406240950851.png?token=ACGIA7QNMQZA62U6POXDG33GPDITW)

SVD 将任意的矩阵 X 分解为 U、S、V 这 3 个矩阵的乘积，其中 U 和 V 是列向量彼此正交的正交矩阵，S 是除了对角线元素以外其余元素均为 0 的对角矩阵。图 2-9 中直观地表示出了这些矩阵。

![](../../attachments/Pasted%20image%2020240925163738.png)

如果矩阵大小是 N, SVD 的计算的复杂度将达到 O（N 3）​。这意味着 SVD 需要与 N 的立方成比例的计算量。因为现实中这样的计算量是做不到的，所以往往会使用 Truncated SVD等更快的方法。Truncated SVD 通过截去（truncated）奇异值较小的部分，从而实现高速化

#### 总结：如何将单词向量化
使用语料库，计算上下文中的单词数量，生成共现矩阵，再将它们转化 PPMI 矩阵，再基于 SVD 降维获得稠密的单词向量。每个单词向量可以取前 N 维即降维。
#### 小结   

- 使用 WordNet 等同义词词典，可以获取近义词或测量单词间的相似度等
- 使用同义词词典的方法存在创建词库需要大量人力、新词难更新等问题
- 目前，使用语料库对单词进行向量化是主流方法
- 近年来的单词向量化方法大多基于“单词含义由其周围的单词构成”这一分布式假设
- 在基于计数的方法中，对语料库中的每个单词周围的单词的出现频数进行计数并汇总（=共现矩阵）
- 通过将共现矩阵转化为 PPMI 矩阵并降维，可以将大的稀疏向量转变为小的密集向量
- 在单词的向量空间中，含义上接近的单词距离上理应也更近

## Word2Vec



## RNN

## GatedRNN

## 基于 RNN 生成文本

## Attention

