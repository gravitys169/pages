# 第22章：调度与资源管理对比 (Scheduling & Resource Management Comparison)

调度系统负责将逻辑上的计算任务映射到物理集群资源上，而资源管理则负责资源的申请、分配和监控。这两个组件的效率和策略直接影响集群的利用率、作业的执行性能以及多租户环境下的公平性。本章将对比Spark、Flink和Presto/Trino在调度粒度、资源分配单元以及与外部资源管理器集成方面的设计差异。

## 22.1 调度粒度 (Scheduling Granularity)

调度粒度定义了调度器做出决策的基本单位。

1.  **Spark: Stage / Task**
    *   **逻辑调度 (DAGScheduler):** 以**Stage**为单位进行调度。DAGScheduler根据RDD的依赖关系将作业划分为Stage，并按照依赖顺序提交Stage。
    *   **物理调度 (TaskScheduler):** 以**Task**为单位进行调度。对于每个待执行的Stage，TaskScheduler会为其生成一组Task（通常对应数据分区），并将这些Task分配给可用的Executor。
    *   **特点:** 两层调度。Stage级别的调度决定了执行的先后顺序和依赖关系，Task级别的调度负责将计算任务分配到具体节点。

2.  **Flink: Job / Task (Subtask)**
    *   **作业级调度 (Dispatcher/JobMaster):** 当一个作业提交时，Dispatcher负责启动JobManager（如果需要），JobManager则负责该作业的整个生命周期，包括向ResourceManager申请资源（Task Slot）。
    *   **任务级调度 (JobMaster/Scheduler):** JobMaster根据ExecutionGraph将作业的并行任务（Subtask）部署到TaskManager上分配到的**Task Slot**中。一旦部署，Task通常会持续运行（流处理模式）。
    *   **特点:** 以Task Slot为资源单位进行调度，调度决策发生在作业启动时（Eager Scheduling）或Task需要运行时（Lazy from Source）。流处理模式下，Task一旦调度成功就长期运行。

3.  **Presto/Trino: Query / Stage / Task (Split)**
    *   **查询级调度 (Coordinator):** Coordinator接收查询后，会对其进行排队（基于Resource Group等），并规划整个查询的执行计划（划分为Stage）。
    *   **Stage/Task级调度 (Coordinator/NodeScheduler):** Coordinator中的调度器 (e.g., `NodeScheduler`) 负责将Stage中的Task（处理一个或多个Split）分配给可用的Worker节点。调度决策会考虑数据本地性（由Connector Split提供）。
    *   **特点:** 多层次调度。查询首先被接纳和规划，然后Stage按流水线顺序调度，每个Stage内的Task被分配到Worker执行。调度是动态的，贯穿查询的整个生命周期。

**调度粒度对比:**

| 引擎        | 主要调度单元                     | 调度层次/组件                                   | 特点                                                         |
| :---------- | :------------------------------- | :---------------------------------------------- | :----------------------------------------------------------- |
| **Spark**   | Stage, Task                      | DAGScheduler (Stage), TaskScheduler (Task)      | 两层调度，Stage决定顺序，Task分配到Executor                     |
| **Flink**   | Job, Task (Subtask) / Task Slot | Dispatcher/JobMaster (Job), JobMaster/Scheduler (Task) | 作业启动时申请Slot，Task部署到Slot，流模式下长期运行          |
| **Presto/Trino**| Query, Stage, Task (Split)       | Coordinator (Query, Stage), NodeScheduler (Task) | 多层动态调度，查询排队，Stage流水线调度，Task分配到Worker         |

## 22.2 资源分配策略 (Resource Allocation Strategy)

引擎如何定义和分配计算资源。

1.  **Spark: Executor Container / Core**
    *   **资源单元:** Spark通过**Executor**来执行任务。每个Executor运行在一个由资源管理器（YARN, K8s）分配的**Container**中。Executor拥有一定数量的CPU核心（Cores）和内存。
    *   **分配方式:** Driver向资源管理器申请若干个Executor（指定核数和内存）。资源管理器分配Container后，Executor启动并向Driver注册。TaskScheduler将Task分配给Executor上的可用**Core**。
    *   **动态分配:** Spark支持动态资源分配，可以根据负载自动增减Executor数量。
    *   **特点:** 以进程（Executor）作为资源隔离和管理的基本单位，细粒度任务（Task）在Executor的线程（Core）上执行。

2.  **Flink: Task Slot**
    *   **资源单元:** Flink定义了**Task Slot**作为资源分配的基本单位。每个TaskManager启动时会向JobManager汇报它拥有的Task Slot数量（通常配置为等于CPU核数）。
    *   **分配方式:** JobManager的ResourceManager负责管理可用的Task Slot。当作业需要执行时，JobManager会向ResourceManager申请所需数量的Slot。Scheduler将作业的并行Task（Subtask）部署到这些分配到的Slot中。
    *   **Slot Sharing:** 默认情况下，来自**同一个作业**的不同算子的Task可以共享同一个Slot，以提高资源利用率，前提是它们是链式调用的（Operator Chain）。来自不同作业的Task不能共享Slot。
    *   **特点:** 以Task Slot作为细粒度的资源划分和调度单位，提供了作业间的资源隔离，并通过Slot Sharing提高作业内资源利用率。

3.  **Presto/Trino: Worker Node / Core (Implicit)**
    *   **资源单元:** Presto/Trino的资源管理相对简单。集群由固定数量（或可通过集群管理工具调整）的**Worker节点**组成。每个Worker节点拥有一定的计算能力（CPU, 内存）。
    *   **分配方式:** Coordinator将Task直接分配给符合条件的**Worker节点**。Worker内部通过线程池来执行这些Task。它不依赖外部资源管理器进行细粒度的Container或Slot分配。
    *   **资源组 (Resource Groups):** Presto/Trino通过Resource Group机制来控制查询的并发度、内存使用和CPU时间，实现多租户环境下的资源限制和优先级管理。
    *   **特点:** Worker节点是资源分配的基本物理单元，没有显式的逻辑资源单元（如Slot）。资源控制主要通过Resource Group在查询级别进行管理。

**资源分配对比:**

| 引擎        | 资源分配单元              | 分配机制                                      | 资源隔离/共享                                  | 特点                                                       |
| :---------- | :------------------------ | :-------------------------------------------- | :--------------------------------------------- | :--------------------------------------------------------- |
| **Spark**   | Executor Container / Core | Driver申请Container -> Executor启动 -> Task分配Core | 进程级隔离 (Executor)                         | 基于进程，支持动态分配                                       |
| **Flink**   | Task Slot                 | TaskManager提供Slot -> JobManager申请/分配Slot -> Task部署 | Slot提供作业间隔离，Slot Sharing提供作业内共享 | 细粒度逻辑资源单元，利于资源控制和共享                     |
| **Presto/Trino**| Worker Node               | Coordinator直接分配Task到Worker               | 进程级隔离 (Worker)，Resource Group控制查询资源 | 物理节点为单位，通过Resource Group进行查询级资源控制          |

## 22.3 与外部资源管理器集成方式 (Integration with External Managers)

现代大数据引擎通常运行在YARN或Kubernetes等通用资源管理器之上。

1.  **Spark:**
    *   **YARN:** 非常成熟的集成。Driver可以运行在YARN的Application Master (AM) 中（`cluster`模式）或客户端（`client`模式）。Executor在YARN分配的Container中运行。Spark通过YARN的API申请和管理资源。
    *   **Kubernetes:** 官方原生支持。Driver和Executor都运行在Pod中。Spark利用Kubernetes API创建、监控和删除Pod，支持动态资源分配。
    *   **Standalone:** 自带的简单集群管理器。
    *   **特点:** 与主流资源管理器紧密集成，利用其能力进行资源调度和隔离。

2.  **Flink:**
    *   **YARN:** 支持多种模式（Session, Per-Job, Application Mode）。JobManager和TaskManager运行在YARN Container中。Flink与YARN交互申请/释放Container（代表TaskManager或JobManager）。Application Mode下，AM即JobManager。
    *   **Kubernetes:** 官方原生支持。提供Session和Application Mode。JobManager和TaskManager运行在Pod中。利用K8s进行部署、服务发现、配置管理等。
    *   **Standalone:** 自带集群模式。
    *   **特点:** 与YARN/K8s深度集成，特别是Application Mode简化了云原生部署。

3.  **Presto/Trino:**
    *   **Standalone (主):** 主要设计为独立部署的集群服务，不直接强依赖YARN或K8s进行自身的资源调度（虽然可以借助它们部署Coordinator和Worker进程）。
    *   **YARN/Kubernetes (部署辅助):** 可以使用YARN或Kubernetes来**部署和管理**Presto/Trino的Coordinator和Worker进程/容器，例如使用Slider (YARN) 或 Helm Charts (Kubernetes)。但这主要是为了**进程管理和生命周期控制**，而不是让Presto/Trino使用YARN/K8s进行内部的Task Slot或细粒度资源分配。
    *   **特点:** 通常独立部署，可借助外部管理器管理进程，但内部调度不直接依赖外部管理器的资源单元。

**集成对比:**

| 引擎        | YARN集成方式                       | Kubernetes集成方式                 | 主要依赖关系                                       |
| :---------- | :--------------------------------- | :--------------------------------- | :----------------------------------------------- |
| **Spark**   | 紧密集成 (Client/Cluster Mode)     | 原生支持 (Driver/Executor in Pods) | 依赖外部管理器进行Container/Pod分配与管理         |
| **Flink**   | 紧密集成 (Session/Per-Job/App Mode)| 原生支持 (Session/Application Mode) | 依赖外部管理器进行Container/Pod分配与管理         |
| **Presto/Trino**| 部署辅助 (e.g., Slider)            | 部署辅助 (e.g., Helm Charts)       | 主要用于进程/服务管理，内部调度相对独立           |

**总结:** Spark和Flink深度集成了主流的资源管理器（YARN, Kubernetes），利用它们进行资源的申请、分配和隔离。它们的调度粒度和资源单元设计也与这些管理器紧密配合。Presto/Trino则更倾向于独立部署，其内部调度和资源管理（通过Resource Groups）相对独立于外部管理器，外部管理器更多用于部署和运维层面。这些差异反映了它们不同的设计目标：Spark和Flink作为通用的计算平台需要适应各种部署环境，而Presto/Trino作为专用的查询服务则优先考虑自身的MPP调度效率。 