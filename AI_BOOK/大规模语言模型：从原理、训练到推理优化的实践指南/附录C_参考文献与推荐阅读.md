# 附录C：参考文献与推荐阅读 (References and Further Reading)\n\n本附录旨在列出本书在撰写过程中参考的重要文献、博客文章、代码库以及推荐给希望深入学习的读者的扩展阅读材料。参考文献将大致按照章节主题进行组织，并可能包含一些贯穿全书的通用资源。\n\n## 通用资源与综述 (General Resources & Surveys)\n\n*   **Speech and Language Processing (3rd ed. draft)** by Dan Jurafsky and James H. Martin. (经典的自然语言处理教材，涵盖了众多基础知识) [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/)\n*   **Deep Learning** by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. (深度学习领域的权威著作) [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)\n*   **Hugging Face Platform:** (模型、数据集、工具、课程的核心平台) [https://huggingface.co/](https://huggingface.co/)\n*   **arXiv cs.CL (Computation and Language):** (获取最新 NLP 和 LLM 论文预印本的主要来源) [https://arxiv.org/list/cs.CL/recent](https://arxiv.org/list/cs.CL/recent)\n\n## 第一部分：基础篇 (Part 1: Foundations)\n\n### 第1章：初识大语言模型 (Decoding LLMs)\n\n*   Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). **Language models are few-shot learners**. *Advances in neural information processing systems*, *33*, 1877-1901. (GPT-3 论文，展示了 ICL 和涌现能力)\n*   Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). **Scaling laws for neural language models**. *arXiv preprint arXiv:2001.08361*. (提出了 Scaling Laws)\n*   Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., ... & Fedus, W. (2022). **Emergent abilities of large language models**. *Transactions on Machine Learning Research*. (系统阐述了 LLM 的涌现能力)\n*   Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). **Bert: Pre-training of deep bidirectional transformers for language understanding**. *arXiv preprint arXiv:1810.04805*. (BERT 论文)\n*   Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). **Improving language understanding by generative pre-training**. (GPT-1 论文)\n*   Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). **Language models are unsupervised multitask learners**. (GPT-2 论文)\n*   Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). **Exploring the limits of transfer learning with a unified text-to-text transformer**. *Journal of Machine Learning Research*, *21*(140), 1-67. (T5 论文)\n*   Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., ... & Lample, G. (2023). **LLaMA: Open and efficient foundation language models**. *arXiv preprint arXiv:2302.13971*. (LLaMA 1 论文)\n\n### 第2章：夯实内功 (Essential Deep Learning and NLP)\n\n*   Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). **Efficient estimation of word representations in vector space**. *arXiv preprint arXiv:1301.3781*. (Word2Vec 论文)\n*   Pennington, J., Socher, R., & Manning, C. D. (2014). **Glove: Global vectors for word representation**. *Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)*, 1532-1543. (GloVe 论文)\n*   Hochreiter, S., & Schmidhuber, J. (1997). **Long short-term memory**. *Neural computation*, *9*(8), 1735-1780. (LSTM 论文)\n*   Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). **Learning phrase representations using RNN encoder-decoder for statistical machine translation**. *arXiv preprint arXiv:1406.1078*. (GRU 与 Seq2Seq 早期工作)\n*   Bahdanau, D., Cho, K., & Bengio, Y. (2014). **Neural machine translation by jointly learning to align and translate**. *arXiv preprint arXiv:1409.0473*. (引入 Attention 机制到 Seq2Seq)\n\n### 第3章：Transformer 深度解析 (The Transformer Engine)\n\n*   Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). **Attention is all you need**. *Advances in neural information processing systems*, *30*. (Transformer 奠基之作)\n*   Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). **Outrageously large neural networks: The sparsely-gated mixture-of-experts layer**. *arXiv preprint arXiv:1701.06538*. (早期 MoE 工作)\n*   Fedus, W., Zoph, B., & Shazeer, N. (2021). **Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity**. *arXiv preprint arXiv:2101.03961*. (Switch Transformer 论文)\n*   Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. D. L., ... & Sayed, W. E. (2024). **Mixtral of experts**. *arXiv preprint arXiv:2401.04088*. (Mixtral 论文)\n*   Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). **Transformer-xl: Attentive language models beyond a fixed-length context**. *arXiv preprint arXiv:1901.02860*. (引入相对位置编码等改进)\n*   Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2021). **Roformer: Enhanced transformer with rotary position embedding**. *arXiv preprint arXiv:2104.09864*. (RoPE 论文)\n 