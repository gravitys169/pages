#myblog
# Overview

大数据引擎是处理海量数据的主要抓手，可以一句话概括为：将用户意图sql解析为逻辑计划，再具体化为物理算子，用于处理不同业务逻辑，最终给出结果的工具

典型的大数据处理流程，以开源Spark的执行流程为例：
![[attachments/大数据分析算子算法分析 2025-01-17 14.53.45.excalidraw|30%]]
其中物理执行部分是典型的CPU intensive负载，一般需要分布式执行，消耗数百到数千不等的CPU
本文主要聚焦于物理执行的各算子的算法介绍，希望通过本文讲通讲透大数据算子的常见实现算法与优化方法。

大数据算子的实现过程中涉及许多细节，比如如何根据负载特征选取合适算子算法，如何处理null值，如何组织内存等等，需要考虑的细节很多。
由于用户负载多变，我们在实践过程中，也根据不同负载针对性的进行了很多算子算法优化。
如果大数据引擎在某个客户负载下表现较差，可能就是因为客户负载的独特性而引擎尚未支撑或代码未优化的缘故。

本文介绍的大数据算子算法思想来自多个引擎，主要对比了Spark与Presto，不过算法同样适用于Clickhouse、Doris与DuckDB等分析引擎。

PS：要想sql执行的快，加快算子执行速度是一个重要方向，而如何组织这些算子，即执行计划的优化往往发挥着更大的作用。
两者的关系类似与手脚和大脑的关系，优秀的大脑可以让手脚少做很多无用功，不过执行计划的优化不是本文的关注点
#### 总结而言
正如CPU执行分前段和后端一样，对应到算子也针对性的分为这两类优化：
1. 后端内存优化
2. 前段执行优化
# 内存数据结构

#### 定长数据类型
Vector的主要成员变量为类型数组与bool数组
支持原始类型包括：boolean，int，long，double，decimal
![[attachments/大数据分析算子算法分析 2025-01-16 16.52.46.excalidraw]]
使用bool数组而不是null bitmap相当于使用内存换时间
#### Variable width vector
变长数据类型顾名思义，需要额外的offset数组来用于计算变长类型的长度
![[attachments/大数据分析算子算法分析 2025-01-16 17.06.53.excalidraw]]
#### Dictionary Encoding Vector
字典编码包含一个原始vector以及一个ids数组，ids的大小为真正的该vector的大小
字典编码的好处：
1. 当原始vector很小而ids很大时，可以节约内存空间（字典编码同样可以用在持久化存储，如ORC/Parquet格式上以节约磁盘）
2. 减少数据的复制，节约内存复制的计算开销
字典一般用在join operator的build侧输出或filter输出
![[attachments/大数据分析算子算法分析 2025-01-16 20.24.24.excalidraw|40%]]
#### VectorBatch
多个vector组成VectorBatch，Vectorbatch 可以认为是一张表的N 行数据的切片，且数据是列式存储的，这区别于传统的关系型数据库行式存储（OLTP数据库往往仅查询一行数据，较少多行计算，便于通过主键得出整行数据）
行存和列存在逻辑上都可以看做一张表，在物理内存的获取上存在差异而已
![[attachments/大数据分析算子算法分析 2025-01-17 14.21.57.excalidraw]]
#### 内存申请
一般而言，Vector的类型数组封装在buffer中，buffer通过单独的allocator申请内存
![[attachments/大数据分析算子算法分析 2025-01-16 17.30.18.excalidraw|20%]]
特别地，针对变长类型，多个字符串是连续存储在一块buffer中（buffer可以是std::vector形式char，也可以是char*，而基础类型是Raw_type*），**不支持乱序赋值；**
当buffer大小不足以存储最新的字符串时，将buffer扩容一倍，并copy旧buffer到新buffer中
一种避免复制的方式是根据varchar(length)，直接申请size*length的内存，但这样势必存在较大内存浪费。

另一种变长类型的内存管理方式是离散化:
![[attachments/大数据分析算子算法分析 2025-01-16 20.13.12.excalidraw|30%]]

# 算子执行模式

算子执行的经典模式 是火山模型
所谓火山模型的本质是一个operator的输出是另一个operator的输入，一般通过迭代器模式的next（Spark）或者addInput/getOutput（Presto）来实现
在数据库中火山模型的输入是one row，而在大数据引擎中，输入为一个vectorbatch，便于对每个vector进行批量操作，顾常被叫做 **“向量化执行”模式** 

![[attachments/大数据分析算子算法分析 2025-01-17 11.49.05.excalidraw|10%]]

这里的operator一般是通过多态实现的，代码上是通过父类operator来操作各子类，因而存在一定的虚函数开销。
为了减少虚函数的开销，产生了另外一种执行模式：codegen
即运行时根据算子种类、算子处理的数据类型等信息，直接生成stage级别的算子，在spark中叫做wholestage codegen
除了减少虚函数开销，代码生成还有利于cache：上个算子处理外的数据可以尽快被下游算子处理，减少了内存物化 

#### Pipeline与算子吞吐
Pipeline设计理念基本贯穿了计算机系统设计的各个领域，典型的如CPU的流水线设计。
SQL引擎的算子执行也采用Pipeline理念。

![[attachments/万字长文一次性搞懂常见大数据算子算法 2025-01-21 20.37.22.excalidraw|40%]]
##### 单个算子吞吐提升如何影响整体Pipeline性能
假设从磁盘读取100GB，Scan由于做了优化（比如data cache等）吞吐从10GB/s提升到20GB/s，那么scan算子将加速5s
**下游算子由于输入数据变快了，比如filter也将加速5s吗？----这是完全错误的**
filter算子处理的数据不变，那么其处理时间则是固定的，除非提升filter算子的性能。
也许你会说，filter吞吐提升了，原来等待的时间现在没有了，当然加速了呀
其实filter打不满吞吐导致等待的时间，正是因为scan慢，这个时间其实已经算在scan的执行时间里。
**每个算子都只能提升自己的性能，而无法提升其他算子的执行性能**
另一个典型的例子是spark的Shuffle算子，shuffle在stage间落盘导致性能影响严重，而presto将shuffle也做成了Pipeline如上图，那么加速的也只是shuffle的时间
##### 单个算子吞吐下降如何影响整体Pipeline性能
根据上下游交互的不同，可以分为shuffle和非shuffle类算子两类。
- 非shuffle类算子：下游算子通过addInput拉取上游算子的getOutput，下游算子如果吞吐下降，那么执行addInput的频率下降，那么上游getOutput的次数将相应下降，但getOutput处理的时间并不会变
- shuffle类算子：shuffle sink通过cient-server架构拉取shuffle output结果，在client和server端各有一个buffer，如果shuffle下游算子执行慢了，为了防止sink buffer爆仓，从而会降低拉取shuffle output的速度，相应的output为了避免buffer爆仓，也会降低addInput执行频率，这是一种常见的全内存计算架构的背压机制，但需要注意的是算子addInput/getOutput处理的时间并不会变。
下游算子如果处理效率降低，这会导致该算子吞吐下降，但并不影响Pipeline性能。

#### [代码example](https://github.com/gravitys169/train/tree/main/java/operator-demo)
###### Case 1： 每个算子处理速度相同
每个算子的执行时间通过sleep time模拟，均为5ms，处理100个Page，每个算子处理时间约为500ms，总处理时间约为2000ms（由各算子的处理时间相加）
实际吞吐=Page number/(last page time - frist page time)
理论吞吐=page number/processing time

| 算子类型     | 处理页面数 | 输入时间(ms/页) | 输出时间(ms/页) | 总时间(ms/页) | 总处理时间(ms) | 实际吞吐(页/秒) | 理论吞吐(页/秒) | 执行时间占比(%) |
| -------- | ----- | ---------- | ---------- | --------- | --------- | --------- | --------- | --------- |
| Source   | 100   | 0.00       | 5.14       | 5.14      | 514       | 48.96     | 194.55    | 25.20     |
| Process1 | 100   | 0.00       | 5.10       | 5.10      | 510       | 48.94     | 196.08    | 25.00     |
| Process2 | 100   | 5.08       | 0.00       | 5.08      | 508       | 48.94     | 196.85    | 24.90     |
| Sink     | 100   | 0.00       | 5.08       | 5.08      | 508       | 48.94     | 196.85    | 24.90     |
###### Case 2：上游算子处理速度加快
source算子执行速度提升5倍（代码中将sleep时间缩短为1ms）

| 算子类型     | 处理页面数 | 输入时间(ms/页) | 输出时间(ms/页) | 总时间(ms/页) | 总处理时间(ms) | 实际吞吐(页/秒) | 理论吞吐(页/秒) | 执行时间占比(%) |
| -------- | ----- | ---------- | ---------- | --------- | --------- | --------- | --------- | --------- |
| Source   | 100   | 0.00       | 1.04       | 1.04      | 104       | 61.15     | 961.54    | 6.36      |
| Process1 | 100   | 0.00       | 5.10       | 5.10      | 510       | 61.11     | 196.08    | 31.19     |
| Process2 | 100   | 5.15       | 0.00       | 5.15      | 515       | 61.11     | 194.17    | 31.50     |
| Sink     | 100   | 0.00       | 5.06       | 5.06      | 506       | 61.15     | 197.63    | 30.95     |

###### Case 3：下游算子处理速度下降
sink算子执行速度下降4倍

| 算子类型     | 处理页面数 | 输入时间(ms/页) | 输出时间(ms/页) | 总时间(ms/页) | 总处理时间(ms) | 实际吞吐(页/秒) | 理论吞吐(页/秒) | 执行时间占比(%) |
| -------- | ----- | ---------- | ---------- | --------- | --------- | --------- | --------- | --------- |
| Source   | 100   | 0.00       | 5.11       | 5.11      | 511       | 28.27     | 195.69    | 14.48     |
| Process1 | 100   | 0.00       | 5.07       | 5.07      | 507       | 28.27     | 197.24    | 14.37     |
| Process2 | 100   | 5.09       | 0.00       | 5.09      | 509       | 28.27     | 196.46    | 14.43     |
| Sink     | 100   | 0.00       | 20.01      | 20.01     | 2001      | 28.27     | 49.98     | 56.72     |

###### Case 4：中间算子处理速度下降
process1和process2算子执行速度下降3倍

| 算子类型     | 处理页面数 | 输入时间(ms/页) | 输出时间(ms/页) | 总时间(ms/页) | 总处理时间(ms) | 实际吞吐(页/秒) | 理论吞吐(页/秒) | 执行时间占比(%) |
| -------- | ----- | ---------- | ---------- | --------- | --------- | --------- | --------- | --------- |
| Source   | 100   | 0.00       | 5.08       | 5.08      | 508       | 24.76     | 196.85    | 12.60     |
| Process1 | 100   | 0.00       | 15.07      | 15.07     | 1507      | 24.77     | 66.36     | 37.37     |
| Process2 | 100   | 15.08      | 0.00       | 15.08     | 1508      | 24.77     | 66.31     | 37.39     |
| Sink     | 100   | 0.00       | 5.10       | 5.10      | 510       | 24.77     | 196.08    | 12.65     |

# 典型算子算法
## Scan
## FilterAndProject
filter和project算子本质上都是求解一个表达式的过程
由于其表达式可能非常复杂，遍历表达式与数据类型需要大量分支判断且性能不高，同时支持范围有限，因而软件上往往采取code gen的方式。
codegen可以有效避免数据类型判断、分支判断与虚函数等操作，能有效提升分析性能。
#### 代码生成过程
![[attachments/万字长文一次性搞懂常见大数据算子算法 2025-01-20 19.42.56.excalidraw|40%]]

llvm生成的函数都是逐行处理的，很难生成向量化的函数。
在向量化执行引擎中，代码生成对列式数据结构并不友好。

不过代码生成的编译时间较长（特别是开启了较多编译优化的时候），如果sql执行时间较短，那么代码生成可能得不偿失
#### UDF
UDF是为了满足用户的业务逻辑而编写的自定义函数，主要包括：
1. 单进单出的UDF，用的最多
2. 多进单出的UDAF，聚合函数
3. 单进多出的UDTF，不过用的很少

大数据引擎往往都会提供UDF框架与接口，用户只需实现具体的接口即可，如典型的Hive/Spark UDF，只需实现evaluate接口就可嵌入引擎进行分布式计算
##### 跨引擎UDF
当需要在不同引擎间支持存量UDF，往往面临较大挑战，当前主流的方法有：
1. 对于同一种语言的引擎，例如Presto支持Hive UDF，可以在presto中提供框架，获取hive udf定义并注册到presto引擎中，反射调用hive udf的evaluate方法
2. 为了避免第三方udf的安全与运行时问题，可以通过搭建remote udf server，通过rpc获取udf结果，trick的地方在于udf server要搭建原UDF的
3. 跨语言的引擎，例如Native Engine支持Java Hive UDF，可以通过在native层codegen出函数，然后在native侧通过JNI构建JVM环境，再反射调用evaluate方法
4. 还有一种很fancy的编译器方案，比如直接将Java Hive UDF通过编译器编译成C++ function，从而在C++侧执行，**这对编译器是巨大的挑战，不过随着LLM的火热，使用LLM代替编译器进行翻译，对于LLM可以说不费吹灰之力，毕竟什么语言在LLM看来都不过是一个个向量token，做翻译是大模型最擅长的事**

## HashAgg

简而言之，根据某些列进行分组，然后根据分组后的结果，针对部分列做聚合运算。
这个过程主要通过实现一个高度优化、定制化的hashtable来完成。
![[attachments/大数据分析算子算法分析 2025-01-17 15.19.31.excalidraw|40%]]
#### 算法过程
OmniRuntime HashAgg 算子的实现主要步骤如下：

1. 输入一个vectorbatch，计算 group by 列的hash 值，根据group by列的个数与大小可以分为三种情况：
	1. group size>16B，则序列化为字符串，再求hash
	2. <2B, 直接强转为int16，不用计算hash
	3. <=16B, 强转为int32[4]， 对这个数组求hash
2. hash值对数组size取模后

    a. 如果该槽位为空，则直接将序列化值、初始化state以及hash值，放入该槽位

    b. 如果槽位不为空，则比较其真实数据  

        i. 如果真实数据相等（比较序列化后的值，原始vector可以释放），则将其state值的引用放入State Array对应vector的行号，以便后期一次性计算整个VectorBatch。注意：这里没有采用就地更新state的方法  

        ii.如果真实数据不等，将槽位+1，重复步骤a 判断下一个槽位的状况  

3. 根据 AggStateArray 批量计算一个 vectorBatch
	1. 针对每个state，构建一个与vector同行数的vector state array，array引用state的值
	2. 针对vector和state array进行向量化计算
4. 释放本vectorbatch，输入下一个vectorbatch；在所有Batch处理完成后，deserialize hashtable Key获得group by的Key列，同时输出state
#### Note
1. 在Hashtable的处理过程中，涉及到两个扩容以及对应的数据copy，如何复用内存，减少memory allocate是提升性能的关键
	1. hashtable array slot不够时，如何扩容
	2. 存储Key序列化值的chunk的扩容
	3. key每次都需要序列化，且申请内存来自chunk，如果该key已经在hashtable中，那么涉及到将该序列化内存释放以便后续复用，否则在低基数场景内存开销非常大
2. HashAgg的向量化
	1. agg的状态更新十分适合向量化，即一个add/max指令完成一个vector多行计算
	2. 典型向量化指令：

| base type | neon type   | init        | load      | compute   |
| --------- | ----------- | ----------- | --------- | --------- |
| int16_t   | int16x8_t   | vdupq_n_s16 | vld1q_s16 | vaddq_s16 |
| int32_t   | int32x4_t   | vdupq_n_s32 | vld1q_s32 | vaddq_s32 |
| double    | float64x2_t | vdupq_n_f64 | vld1q_f64 | vaddq_f64 |


## HashJoin

Join 是指将两张表根据 join key 进行关联，并输出某些列的操作。
presto 在传给 LookupJoinOperatorFactory 的类中，附带了所有的 build 侧的 hashtable 信息，在 probe 时，LookupJoinOperator 需要找到对应分区的 hashtable 进行 join。
而 Spark 的实现则简单一些，一个 LookupJoinOperator 对应一个 hashtable，且当 hashtable 完成后，lookupJoinOp 才开始构建。
二者的区别是presto一个task可能要处理多个分区，而spark一个task仅处理一个分区

HashJoin 算子主要分为两个阶段：build和probe

#### 构建 HashTable
![[attachments/大数据分析算子算法分析 2025-01-20 10.53.50.excalidraw|40%]]
##### 算法过程
1. 接受输入 VB，构建出 PageIndex，即编码得到valuesAdress
2. 计算每列的 join key 的 hash 值，如果多列，则求 combineHash 值
3. 建立 hash 表的核心数据结构 int* slot，查看slot[hash]
	1. 如果数组的槽位没被填充，则直接令 slot[hash]=VA
	2. 该槽位被填充
		1. 如果Key列数据真实相等，令 key[hash]=新 VA ，同时填充 positionLinks[新va]=旧va
		    1. 这里构造了一个positionLinks来存储key值相同的行，也可以采用更直接的方法，不使用valueAddress，直接保存batch号和行号，多个重复值构成一个数组
		   **note：在软件实现中，针对同一功能，经常存在数组与链表数据结构的不同实现**
		1. 如果不相等，则递增 hash 值，试探下一槽位  
###### 基于workload的array table优化
基于hash构建hash table是最常见的实现，但如果我们对workload有一定了解，或者可以通过较小抽样成本获取workload特征，那么就可以构造出非hash的hashtable
**由于join的build table需要累积所有数据，这是与hash agg的hash table的核心不同，因而可以很好的profile workload**， 如果key值为整型，且其range较小时，则可以直接将（key值-min）作为slot下标，从而避免hash计算。
如上图的hash table可以简化为，其他不变：![[attachments/大数据分析算子算法分析 2025-01-20 11.25.44.excalidraw]]
#### Probe  过程
##### 算法过程
![[attachments/一文搞懂大数据分析算子算法 2025-01-20 11.58.33.excalidraw|40%]]

1. probe 侧输入一个 vectorBatch，计算出整个 vectorBatch 列对应 join key 的 hash 值
2. 对 probe 的每一行，计算出 probe 行的 hash 值，如果build侧 slot[hash]不为空，且probe侧值与 build侧 数据行侧相同
	1. 如果有 join 条件为表达式，则解析出 build 和 probe 的值，通过 simplefilter 进行计算
3. 记录下 probe 和 build 的行号信息，以便后续构造使用
4. 如果 positionLinks[va]的 value 不为空，则获取该 value  重复 Step2的值比较
5. probe侧所有vector Batch都计算完成后，收集 probe 和 build 侧的数据，构造输出vectorbatch
	1. probe 侧数据根据选中的情况，分为
		1. 完全复用：即所有行都join上
		2. slice 部分数据：部分连续的行join上
		3. dict 输出：离散的部分行join上，这是最常见的情况
	2. build 侧根据 step3 记录的 VA 信息，解析出行数据，并复制到 output vector 中   
 
## HashTable 归一
由于HashAgg与HashJoin均是基于hashtable完成主要计算逻辑的算子，因此有必要考虑二者共用同一套hashtable，以以便简化维护代价
![[attachments/万字长文一次性搞懂常见大数据算子算法 2025-01-21 17.16.07.excalidraw|40%]]
#### 共同点
1. hashtable 计算hash的方式相同，根据key 列的size来选择是否序列化，一般采用crc32 hash或者MurmurHash
#### 不同点
1. 扩容：HashAgg处理一个batch后即丢弃，Hashtable包含了所有输出需要的信息，因而起始Slot 数组较小，随着输入增多需要扩容，一般采用开放地址法。而hashjoin因为会累积所有数据以用于输出，所以是可以直接根据一定比例，设定slot数组的大小。
2. 重复key：hashagg中重复key的行，就是需要计算state的行，而join中重复Key的行是需要输出的行
3. null：hashagg中空值为一个组别，需要计算state，而join中只要key有一列为null，那么就不应该join上

## Sort  
![[attachments/大数据分析算子算法分析 2025-01-20 09.56.19.excalidraw|40%]]

Sort 主要对全量数据进行排序，其重点是如何在大数据量下组织数据进行排序。
#### 算法过程
Sort 算子主要需要累积数据，然后根据 Sort Key 进行快排。

1. 累积数据的工作主要由 PageIndex 完成，其通过将不同 VectorBatch 统一编码到一个叫 valueAddress 的数组来实现对所有数据行的访问。
	1. 使用valueAddress可以使用较小的内存快速定位到Batch中的一行，一种更浅显的做法直接保存batch号和行号（clickhouse）
2. 然后构造一个 valuePtrs pair，first 为排序 key，second 为 valueAddress，进而全局排序
3. 针对多个排序列的情况，在第一列已经排好的情况下，对第一列的相同值，刷新 pair 的 first 为第二排序列的值，继续排序
4. 最后根据排序后的pair，decode valueAddress构造输出Vector

## SortMergeJoin

#### 算法过程
![[attachments/一文搞懂大数据分析算子算法 2025-01-20 14.36.43.excalidraw|40%]]
sort merge join主要涉及3个步骤：
1. 待join的两表的全量排序
2. 游标根据join key在两表上移动
3. join匹配，输出结果
#### 注意点
1. SMJ的分布式执行需要在shuffle时根据range进行分区，将同一range的数据分到同个节点merge
2. sort一般在stage1完成，merge join一般在stage2完成，两个阶段都涉及到不同vector batch的value Address表示，因此需要考虑复用
## BroadCastJoin
上述两种join算法适用于join的build和probe表都较大的时候，如果其中一个表较小，那么可以将小表广播到各task中，直接进行小表和大表的join，以避免hash运算以及shuffle操作，从而大幅提升性能。
#### 算法过程
![[attachments/万字长文搞懂常见大数据分析算子算法 2025-01-20 17.34.54.excalidraw|40%]]

## Shuffle

shuffle是对数据按照一定规则（hash分桶或排序range）进行分组的算子，以便下游算子按照分组进行计算。
shuffle一般用于切分stage，在spark中也是可靠性的承载算子。
对于窄依赖的算子一般不切分stage，需要切分stage的一般是宽依赖的算子，即下游算子的输入来自多个executor的多个task
**对于宽依赖的算子，如groupbykey、reducebykey、join、sortbykey，如果基于index与file结构shuffle，那么需要等待上游stage的所有task都执行完成后，方可以开始下游task**。

**可以说，shuffle算子是Spark与Presto的最大不同，二者存在根本性区别，而其他算子二者则都存在相互借鉴。
而Clickhouse从立项伊始就对shuffle，特别是shuffle join支持不够**
#### 基于线程的hash shuffle
![[attachments/一文搞懂大数据分析算子算法 2025-01-20 14.58.09.excalidraw|40%]]
算法实现简单直接，每个task线程维度，按hashagg groupby或join key进行hash分桶
最大的问题：单个节点上生成的小文件太多，比如map 200，reduce 200，executor 10，那么生成文件400000个，甚至可能超过linux默认的文件句柄上限（1048576）
##### Note
1. **对于全内存的执行引擎，比如presto，不存在上图的file落盘操作，也就不存在小文件问题**
2. 由于上游stage的输出各自独立，因而下游任务可以即时运行，将不同task的输出当做不同批次的输入，无需等待所有task执行完成，从而形成pipeline，大幅提升性能
#### 基于进程的hash shuffle
![[attachments/一文搞懂大数据分析算子算法 2025-01-20 15.12.51.excalidraw|40%]]
优化了每个task的输出，在executor维度即进程维度，按hashagg groupby或join key进行hash分桶
最大的问题：由于跨线程共享分区buffer，需要多线程操作，增加了锁开销
#### 基于reduce的hash shuffle
![[attachments/一文搞懂大数据分析算子算法 2025-01-20 15.21.38.excalidraw|40%]]
基于进程的hash shuffle主要针对map进行优化，本算法针对的reduce进行优化，通过引入额外的index文件，来记录不同分区的offset
相比而言，减少了多进程锁开销，但引入了额外的index，这是当前主流的hash shuffle方法
#### sort shuffle
![[attachments/一文搞懂大数据分析算子算法 2025-01-20 15.38.06.excalidraw|40%]]

#### Note
无论是hash shuffle还是sort shuffle，储存分区好或排序好数据的buffer的大小非常关键，需要平衡内存占用与性能来设置buffer大小，更大的buffer可以减少生成的file数量以及减少file合并次数，但同时意味着更高的内存占用
在spark中该buffer的设置为spark.shuffle.file.buffer，默认为32KB，在某些内存充足的场景，可以考虑设置为1GB甚至更大，以获得更好的性能，但过大的buffer，可能导致多个executor并行执行时超出节点可用内存，导致进程crash。

## window
#### 传统Window
传统的窗口计算，主要基于分区通过Rank、Sum等窗口函数进行window运算，用于统计窗口内数据。
传统的窗口运算语法大致如下，其中分区、排序、Range与Rows 视具体情况可选
```
Rank/Row_Number/SUM/Max/(column3) OVER (PARTITION BY column1 ORDER BY column2 RANGE/ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS x
```
传统窗口运算，往往是进行分区后，在分区内进行排序、聚合等运算。
#### Flink Window 时间序列Window
典型的流式计算引擎Flink则更加关注的是时间序列的流式计算，也是更符合现实世界人类认知的一类计算。
##### tumbling window
窗口大小固定且不重叠，将数据流按固定长度切分成独立的窗口。常用于对数据进行定期统计，如每小时的销售额统计。
![[attachments/万字长文搞懂常见大数据分析算子算法 2025-01-20 16.44.04.excalidraw]]
##### sliding window
窗口大小固定，但可以重叠。通过指定窗口大小和滑动步长，使得窗口可以以一定间隔在数据流上滑动。
适用于需要在不同时间粒度上进行统计，且统计范围有重叠的场景，**可以提供实时性更好的统计数据**，如每 10 分钟统计过去 1 小时的销售额。
![[attachments/万字长文搞懂常见大数据分析算子算法 2025-01-20 16.51.26.excalidraw]]
#### Flink 窗口Join

# 性能优化
## 向量化
## 内存优化
## 异构加速
