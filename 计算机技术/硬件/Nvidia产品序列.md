NVIDIA's datacenter products are designed to accelerate AI, high-performance computing (HPC), and large-scale data processing. Based on the latest search results, here are some of NVIDIA's key datacenter offerings:

### **1. Blackwell Architecture GPUs**

- **GB300 NVL72**: A rack-scale solution connecting **72 Blackwell GPUs** and **36 Arm Neoverse-based CPUs**, delivering **1.5× more AI performance** than the previous Hopper-based GB200 NVL72 3.

| Configuration           | 72 NVIDIA Blackwell Ultra GPUs, 36 NVIDIA Grace CPUs |
| ----------------------- | ---------------------------------------------------- |
| NVLink Bandwidth        | 130 TB/s                                             |
| Fast Memory             | Up to 40 TB                                          |
| GPU Memory \| Bandwidth | Up to 21 TB \| Up to 576 TB/s                        |
| CPU Memory \| Bandwidth | Up to 18 TB SOCAMM with LPDDR5X \| Up to 14.3 TB/s   |
| CPU Core Count          | 2,592 Arm Neoverse V2 cores                          |
| FP4 Tensor Core         | 1,400 \| 1,100² PFLOPS                               |
| FP8/FP6 Tensor Core     | 720 PFLOPS                                           |
| INT8 Tensor Core        | 23 PFLOPS                                            |
| FP16/BF16 Tensor Core   | 360 PFLOPS                                           |
| TF32 Tensor Core        | 180 PFLOPS                                           |
| FP32                    | 6 PFLOPS                                             |
| FP64 / FP64 Tensor Core | 100 TFLOPS                                           |

    
- **HGX B300 NVL16**: Features **11× faster inference** on large language models (LLMs) and **4× larger memory** compared to Hopper, optimized for AI reasoning and complex workloads 3.
    
- **Blackwell Ultra**: Expected in **H2 2025**, enhancing **training and inference scaling**, particularly for **AI reasoning, agentic AI, and physical AI** applications 3.
    

### **2. Next-Gen GPU Roadmap**

- **Vera Rubin (2026)**: The next architecture after Blackwell, named after the astronomer who discovered dark matter, expected to **lower costs** while boosting AI performance 3.
    
- **Feynman (2027)**: Following Rubin, named after physicist Richard Feynman, continuing NVIDIA’s push for **AI and HPC efficiency** 3.
    

### **3. AI Factory & Inference Acceleration**

- **NVIDIA Dynamo**: Open-source software acting as the **"operating system for AI factories"**, optimizing large-scale AI reasoning and token generation 3.
    
- **DRIVE Thor (2000 TOPS)**: A single-chip solution for **autonomous driving**, unifying AI compute for perception, planning, and control 1.
    

### **4. Robotics & Simulation**

- **Isaac GR00T N1**: The **first humanoid robot foundation model**, enabling generalized reasoning and skills for robotics developers 3.
    
- **Newton Physics Engine**: Open-source physics simulation in collaboration with **Google DeepMind & Disney**, enhancing robot training 3.
    

### **5. DGX AI Supercomputers**

- **DGX Spark & DGX Station**: Powered by **Grace Blackwell**, bringing datacenter-level AI to desktops for local AI model deployment 3.
    

### **6. Data Center Revenue Growth**

- NVIDIA’s **Q3 2025 datacenter revenue reached $30.8B**, a **112% YoY increase**, driven by AI training and inference demand 6.
    

NVIDIA continues to dominate AI infrastructure with innovations in **training (Blackwell), inference (Dynamo), robotics (GR00T), and edge AI (Thor)**. For more details, see the referenced sources.